spark {
    fileFormats {
        defaultFormat = "csv"
    }

    options {
        header = "true"
        inferSchema = "true"
    }

    stream {
        path = "data/temp"
        checkpointPath = "data/temp/checkpoint"
        schema_path = "src/main/resources/stream_schema.conf"
    }

    comprehensiveBankingData {

        path = "src/main/resources/comprehensive_banking_data.csv"

        relevantColumns = [
            "Customer ID",
            "First Name",
            "Last Name",
            "Age",
            "Gender",
            "Email",
            "City",
            "Account Type",
            "Account Balance",
            "TransactionID",
            "Transaction Date",
            "Transaction Type",
            "Transaction Amount",
            "Account Balance After Transaction",
            "Branch ID"
        ]

        rawColNameToProcessedColName = {
           "Customer ID" = "customer_id"
           "First Name" = "first_name"
           "Last Name" = "last_name"
           "Age" = "age"
           "Gender" = "gender"
           "Email" = "email"
           "City" = "city"
           "Account Type" = "account_type"
           "Account Balance" = "account_balance"
           "TransactionID" = "transaction_id"
           "Transaction Date" = "transaction_date"
           "Transaction Type" = "transaction_type"
           "Transaction Amount" = "transaction_amount"
           "Account Balance After Transaction" = "account_balance_after_transaction"
           "Branch ID" = "branch_id"
       }

    }
}

logger {
    path = "logs"
}

monitor {
    path = "monitor-logs"
}